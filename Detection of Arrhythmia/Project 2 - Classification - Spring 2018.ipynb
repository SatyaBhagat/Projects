{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cardiac Arrhythmia Multy-Class Classification \n",
    "\n",
    "Analyze data and address missing data if there is any. \n",
    "\n",
    "Decide aboute a good evaluation strategy and justify your choice. \n",
    "\n",
    "Find the best parameters for the following classification models: \n",
    "- KNN classifcation \n",
    "- Logistic Regression\n",
    "- Linear Supprt Vector Machine\n",
    "- Kerenilzed Support Vector Machine\n",
    "- Decision Tree\n",
    "- Random Forest \n",
    "\n",
    "Then use different bagging and boosting methods to boost the results? Do you see any significant change? Why or why not? \n",
    "\n",
    "Next, use data reduction method you have learned in class to reduce the size of data, and agian try above models. Do you get better results? Justify your answer. \n",
    "\n",
    "<font color = 'red'>Due date for full credit: April 4, 11:59 PM\n",
    "    <br>\n",
    "    Due date for partial credit: April 6, 11:59 PM.\n",
    "    <br> No submission will be accepted after April 6. \n",
    "    <br> Please note that your term paper is also due April 6. \n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>51</td>\n",
       "      <td>100</td>\n",
       "      <td>167</td>\n",
       "      <td>321</td>\n",
       "      <td>174</td>\n",
       "      <td>91</td>\n",
       "      <td>107</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.2</td>\n",
       "      <td>13.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>52</td>\n",
       "      <td>77</td>\n",
       "      <td>129</td>\n",
       "      <td>377</td>\n",
       "      <td>133</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>54</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "      <td>157</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>15.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>56</td>\n",
       "      <td>84</td>\n",
       "      <td>118</td>\n",
       "      <td>354</td>\n",
       "      <td>160</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>30.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>67</td>\n",
       "      <td>89</td>\n",
       "      <td>130</td>\n",
       "      <td>383</td>\n",
       "      <td>156</td>\n",
       "      <td>73</td>\n",
       "      <td>85</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>10.8</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>20.1</td>\n",
       "      <td>25.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>72</td>\n",
       "      <td>102</td>\n",
       "      <td>135</td>\n",
       "      <td>401</td>\n",
       "      <td>156</td>\n",
       "      <td>83</td>\n",
       "      <td>72</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>19.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>86</td>\n",
       "      <td>77</td>\n",
       "      <td>143</td>\n",
       "      <td>373</td>\n",
       "      <td>150</td>\n",
       "      <td>65</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.9</td>\n",
       "      <td>17.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>172</td>\n",
       "      <td>58</td>\n",
       "      <td>78</td>\n",
       "      <td>155</td>\n",
       "      <td>382</td>\n",
       "      <td>163</td>\n",
       "      <td>81</td>\n",
       "      <td>-24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8.8</td>\n",
       "      <td>12.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>73</td>\n",
       "      <td>91</td>\n",
       "      <td>180</td>\n",
       "      <td>355</td>\n",
       "      <td>157</td>\n",
       "      <td>104</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>12.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>48.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>88</td>\n",
       "      <td>77</td>\n",
       "      <td>158</td>\n",
       "      <td>399</td>\n",
       "      <td>163</td>\n",
       "      <td>94</td>\n",
       "      <td>46</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>39.2</td>\n",
       "      <td>54.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>48</td>\n",
       "      <td>75</td>\n",
       "      <td>132</td>\n",
       "      <td>350</td>\n",
       "      <td>169</td>\n",
       "      <td>65</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>17.2</td>\n",
       "      <td>31.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>59</td>\n",
       "      <td>82</td>\n",
       "      <td>145</td>\n",
       "      <td>347</td>\n",
       "      <td>169</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>19.5</td>\n",
       "      <td>41.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>58</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>353</td>\n",
       "      <td>122</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>17.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>63</td>\n",
       "      <td>91</td>\n",
       "      <td>154</td>\n",
       "      <td>392</td>\n",
       "      <td>175</td>\n",
       "      <td>83</td>\n",
       "      <td>73</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>82</td>\n",
       "      <td>181</td>\n",
       "      <td>399</td>\n",
       "      <td>158</td>\n",
       "      <td>79</td>\n",
       "      <td>-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>25.2</td>\n",
       "      <td>38.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>58</td>\n",
       "      <td>83</td>\n",
       "      <td>251</td>\n",
       "      <td>383</td>\n",
       "      <td>189</td>\n",
       "      <td>183</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>9.1</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>17.1</td>\n",
       "      <td>54.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>67</td>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>336</td>\n",
       "      <td>177</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>8.3</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>19.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>75</td>\n",
       "      <td>71</td>\n",
       "      <td>132</td>\n",
       "      <td>364</td>\n",
       "      <td>169</td>\n",
       "      <td>82</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>19.7</td>\n",
       "      <td>34.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>59</td>\n",
       "      <td>75</td>\n",
       "      <td>157</td>\n",
       "      <td>406</td>\n",
       "      <td>143</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>18.4</td>\n",
       "      <td>28.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>55</td>\n",
       "      <td>82</td>\n",
       "      <td>140</td>\n",
       "      <td>388</td>\n",
       "      <td>149</td>\n",
       "      <td>82</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>35.3</td>\n",
       "      <td>57.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>80</td>\n",
       "      <td>109</td>\n",
       "      <td>128</td>\n",
       "      <td>382</td>\n",
       "      <td>195</td>\n",
       "      <td>60</td>\n",
       "      <td>-34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>20.7</td>\n",
       "      <td>29.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>73</td>\n",
       "      <td>94</td>\n",
       "      <td>186</td>\n",
       "      <td>373</td>\n",
       "      <td>224</td>\n",
       "      <td>125</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>44.0</td>\n",
       "      <td>68.4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>54</td>\n",
       "      <td>95</td>\n",
       "      <td>161</td>\n",
       "      <td>407</td>\n",
       "      <td>168</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>54.8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>65</td>\n",
       "      <td>90</td>\n",
       "      <td>164</td>\n",
       "      <td>420</td>\n",
       "      <td>381</td>\n",
       "      <td>99</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>17.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>96</td>\n",
       "      <td>147</td>\n",
       "      <td>400</td>\n",
       "      <td>301</td>\n",
       "      <td>82</td>\n",
       "      <td>-37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-6.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>57</td>\n",
       "      <td>83</td>\n",
       "      <td>164</td>\n",
       "      <td>359</td>\n",
       "      <td>154</td>\n",
       "      <td>69</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>56.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>95</td>\n",
       "      <td>94</td>\n",
       "      <td>203</td>\n",
       "      <td>367</td>\n",
       "      <td>171</td>\n",
       "      <td>106</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>21</td>\n",
       "      <td>140</td>\n",
       "      <td>157</td>\n",
       "      <td>438</td>\n",
       "      <td>226</td>\n",
       "      <td>81</td>\n",
       "      <td>-40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>36.7</td>\n",
       "      <td>115.9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>150</td>\n",
       "      <td>362</td>\n",
       "      <td>177</td>\n",
       "      <td>96</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>52.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>163</td>\n",
       "      <td>340</td>\n",
       "      <td>162</td>\n",
       "      <td>102</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>20.9</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>65</td>\n",
       "      <td>133</td>\n",
       "      <td>148</td>\n",
       "      <td>417</td>\n",
       "      <td>260</td>\n",
       "      <td>92</td>\n",
       "      <td>-158</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>6.4</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>6.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>63</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>364</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>23.7</td>\n",
       "      <td>26.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>24</td>\n",
       "      <td>77</td>\n",
       "      <td>125</td>\n",
       "      <td>358</td>\n",
       "      <td>159</td>\n",
       "      <td>70</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.6</td>\n",
       "      <td>16.1</td>\n",
       "      <td>49.2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>29</td>\n",
       "      <td>123</td>\n",
       "      <td>145</td>\n",
       "      <td>361</td>\n",
       "      <td>221</td>\n",
       "      <td>80</td>\n",
       "      <td>112</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>19.6</td>\n",
       "      <td>-4.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.8</td>\n",
       "      <td>12.2</td>\n",
       "      <td>25.1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>56</td>\n",
       "      <td>79</td>\n",
       "      <td>145</td>\n",
       "      <td>381</td>\n",
       "      <td>173</td>\n",
       "      <td>101</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>20.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>42</td>\n",
       "      <td>88</td>\n",
       "      <td>123</td>\n",
       "      <td>362</td>\n",
       "      <td>228</td>\n",
       "      <td>81</td>\n",
       "      <td>-18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.5</td>\n",
       "      <td>15.1</td>\n",
       "      <td>84.4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>149</td>\n",
       "      <td>290</td>\n",
       "      <td>128</td>\n",
       "      <td>93</td>\n",
       "      <td>-67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>-5.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>-8.3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>65</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>360</td>\n",
       "      <td>163</td>\n",
       "      <td>71</td>\n",
       "      <td>-22</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>62</td>\n",
       "      <td>79</td>\n",
       "      <td>155</td>\n",
       "      <td>367</td>\n",
       "      <td>153</td>\n",
       "      <td>95</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>24.1</td>\n",
       "      <td>33.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "      <td>77</td>\n",
       "      <td>144</td>\n",
       "      <td>340</td>\n",
       "      <td>148</td>\n",
       "      <td>82</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>9.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>19.1</td>\n",
       "      <td>36.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>63</td>\n",
       "      <td>87</td>\n",
       "      <td>142</td>\n",
       "      <td>391</td>\n",
       "      <td>137</td>\n",
       "      <td>88</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.6</td>\n",
       "      <td>43.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>82</td>\n",
       "      <td>88</td>\n",
       "      <td>146</td>\n",
       "      <td>357</td>\n",
       "      <td>179</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>13.5</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>30.1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>66</td>\n",
       "      <td>94</td>\n",
       "      <td>170</td>\n",
       "      <td>383</td>\n",
       "      <td>152</td>\n",
       "      <td>115</td>\n",
       "      <td>92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>33.7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>176</td>\n",
       "      <td>72</td>\n",
       "      <td>88</td>\n",
       "      <td>153</td>\n",
       "      <td>389</td>\n",
       "      <td>172</td>\n",
       "      <td>89</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>24.9</td>\n",
       "      <td>41.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>74</td>\n",
       "      <td>143</td>\n",
       "      <td>374</td>\n",
       "      <td>146</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>40.1</td>\n",
       "      <td>55.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "      <td>85</td>\n",
       "      <td>143</td>\n",
       "      <td>363</td>\n",
       "      <td>146</td>\n",
       "      <td>84</td>\n",
       "      <td>-40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>75</td>\n",
       "      <td>88</td>\n",
       "      <td>157</td>\n",
       "      <td>384</td>\n",
       "      <td>132</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>10.5</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>63</td>\n",
       "      <td>81</td>\n",
       "      <td>143</td>\n",
       "      <td>325</td>\n",
       "      <td>218</td>\n",
       "      <td>74</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>75</td>\n",
       "      <td>91</td>\n",
       "      <td>134</td>\n",
       "      <td>376</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.5</td>\n",
       "      <td>17.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "      <td>57</td>\n",
       "      <td>81</td>\n",
       "      <td>151</td>\n",
       "      <td>363</td>\n",
       "      <td>166</td>\n",
       "      <td>80</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.2</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>17.6</td>\n",
       "      <td>39.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>199</td>\n",
       "      <td>382</td>\n",
       "      <td>154</td>\n",
       "      <td>117</td>\n",
       "      <td>-37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>137</td>\n",
       "      <td>361</td>\n",
       "      <td>201</td>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>62.4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>68</td>\n",
       "      <td>108</td>\n",
       "      <td>176</td>\n",
       "      <td>365</td>\n",
       "      <td>194</td>\n",
       "      <td>116</td>\n",
       "      <td>-85</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>-28.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-44.2</td>\n",
       "      <td>-33.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>55</td>\n",
       "      <td>93</td>\n",
       "      <td>106</td>\n",
       "      <td>386</td>\n",
       "      <td>218</td>\n",
       "      <td>63</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>46.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>70</td>\n",
       "      <td>79</td>\n",
       "      <td>127</td>\n",
       "      <td>364</td>\n",
       "      <td>138</td>\n",
       "      <td>78</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>21.3</td>\n",
       "      <td>32.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>452 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9   ...   270   271   272  \\\n",
       "0     75    0  190   80   91  193  371  174  121  -16 ...   0.0   9.0  -0.9   \n",
       "1     56    1  165   64   81  174  401  149   39   25 ...   0.0   8.5   0.0   \n",
       "2     54    0  172   95  138  163  386  185  102   96 ...   0.0   9.5  -2.4   \n",
       "3     55    0  175   94  100  202  380  179  143   28 ...   0.0  12.2  -2.2   \n",
       "4     75    0  190   80   88  181  360  177  103  -16 ...   0.0  13.1  -3.6   \n",
       "5     13    0  169   51  100  167  321  174   91  107 ...  -0.6  12.2  -2.8   \n",
       "6     40    1  160   52   77  129  377  133   77   77 ...   0.0   6.5   0.0   \n",
       "7     49    1  162   54   78    0  376  157   70   67 ...   0.0   8.2  -1.9   \n",
       "8     44    0  168   56   84  118  354  160   63   61 ...   0.0   7.0  -1.3   \n",
       "9     50    1  167   67   89  130  383  156   73   85 ...  -0.6  10.8  -1.7   \n",
       "10    62    0  170   72  102  135  401  156   83   72 ...  -0.5   9.0  -2.0   \n",
       "11    45    1  165   86   77  143  373  150   65   12 ...   0.0   4.4  -2.2   \n",
       "12    54    1  172   58   78  155  382  163   81  -24 ...   0.0   6.3  -2.1   \n",
       "13    30    0  170   73   91  180  355  157  104   68 ...  -0.9  12.3   0.0   \n",
       "14    44    1  160   88   77  158  399  163   94   46 ...  -0.6  12.4   0.0   \n",
       "15    47    1  150   48   75  132  350  169   65   36 ...   0.0   7.7  -0.8   \n",
       "16    47    0  171   59   82  145  347  169   61   77 ...   0.0   9.4  -1.7   \n",
       "17    46    1  158   58   70  120  353  122   52   57 ...   0.0   6.6   0.0   \n",
       "18    73    0  165   63   91  154  392  175   83   73 ...   0.0   5.7   0.0   \n",
       "19    57    1  166   72   82  181  399  158   79  -12 ...   0.0   7.7  -0.9   \n",
       "20    28    1  160   58   83  251  383  189  183   50 ...  -0.6   9.1  -1.4   \n",
       "21    45    0  169   67   90  122  336  177   78   81 ...  -0.6   8.3  -1.8   \n",
       "22    36    1  153   75   71  132  364  169   82   62 ...   0.0   8.9  -1.0   \n",
       "23    57    1  165   59   75  157  406  143   92    4 ...   0.0   6.7  -0.5   \n",
       "24    40    1  153   55   82  140  388  149   82   52 ...   0.0  13.6   0.0   \n",
       "25    44    0  169   80  109  128  382  195   60  -34 ...   0.0   6.9   0.0   \n",
       "26    34    0  170   73   94  186  373  224  125   90 ...   0.0  15.3  -1.1   \n",
       "27    31    1  160   54   95  161  407  168   83   10 ...   0.0  12.7  -1.8   \n",
       "28    56    1  164   65   90  164  420  381   99   -8 ...   0.0   5.4   0.0   \n",
       "29    51    1  160   83   96  147  400  301   82  -37 ...   0.0   7.3  -3.9   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...   ...   ...   ...   \n",
       "422   29    1  162   57   83  164  359  154   69   64 ...   0.0  14.1  -2.2   \n",
       "423   51    0  186   95   94  203  367  171  106   -7 ...   0.0   9.6  -3.5   \n",
       "424    7    0  119   21  140  157  438  226   81  -40 ...   0.0  10.0  -2.1   \n",
       "425   36    0  171   93   87  150  362  177   96   44 ...   0.0  10.3  -0.8   \n",
       "426   35    1  160   53   55  163  340  162  102   40 ...   0.0   8.7  -0.5   \n",
       "427   58    0  160   65  133  148  417  260   92 -158 ...  -0.4   6.4  -3.5   \n",
       "428   64    0  160   63   83    0  364  120   90   29 ...   0.0   6.7  -0.4   \n",
       "429    8    1  130   24   77  125  358  159   70   87 ...   0.0  11.3  -2.1   \n",
       "430   11    0  138   29  123  145  361  221   80  112 ...  -3.4  19.6  -4.2   \n",
       "431   47    0  166   56   79  145  381  173  101   52 ...   0.0   8.5   0.0   \n",
       "432   11    0  140   42   88  123  362  228   81  -18 ...   0.0  17.1  -7.1   \n",
       "433   70    0  167   60   80  149  290  128   93  -67 ...   0.0   2.7  -5.4   \n",
       "434   20    0  178   65   88  155  360  163   71  -22 ...  -0.5  10.2   0.0   \n",
       "435   39    1  164   62   79  155  367  153   95   50 ...   0.0   9.7  -0.7   \n",
       "436   32    1  164   57   77  144  340  148   82   27 ...  -0.6   9.9  -0.6   \n",
       "437   35    1  155   63   87  142  391  137   88   66 ...   0.0  10.7   0.0   \n",
       "438   37    0  175   82   88  146  357  179   72    1 ...  -0.4  13.5  -1.2   \n",
       "439   49    1  168   66   94  170  383  152  115   92 ...   0.0   8.2  -0.7   \n",
       "440   37    0  176   72   88  153  389  172   89   67 ...  -0.9  16.6  -3.4   \n",
       "441   37    1  160   50   74  143  374  146   75   68 ...   0.0  11.4  -0.9   \n",
       "442   65    1  160   50   85  143  363  146   84  -40 ...   0.0   6.6  -6.1   \n",
       "443   41    1  154   75   88  157  384  132  112   65 ...  -0.4  10.5  -2.5   \n",
       "444   29    0  166   63   81  143  325  218   74   24 ...   0.0   7.8  -1.3   \n",
       "445   45    0  175   75   91  134  376  160   83   91 ...   0.0   7.1  -2.4   \n",
       "446   20    1  157   57   81  151  363  166   80   43 ...   0.0   7.2  -0.7   \n",
       "447   53    1  160   70   80  199  382  154  117  -37 ...   0.0   4.3  -5.0   \n",
       "448   37    0  190   85  100  137  361  201   73   86 ...   0.0  15.6  -1.6   \n",
       "449   36    0  166   68  108  176  365  194  116  -85 ...   0.0  16.3 -28.6   \n",
       "450   32    1  155   55   93  106  386  218   63   54 ...  -0.4  12.0  -0.7   \n",
       "451   78    1  160   70   79  127  364  138   78   28 ...   0.0  10.4  -1.8   \n",
       "\n",
       "     273 274  275  276   277    278  279  \n",
       "0    0.0   0  0.9  2.9  23.3   49.4    8  \n",
       "1    0.0   0  0.2  2.1  20.4   38.8    6  \n",
       "2    0.0   0  0.3  3.4  12.3   49.0   10  \n",
       "3    0.0   0  0.4  2.6  34.6   61.6    1  \n",
       "4    0.0   0 -0.1  3.9  25.4   62.8    7  \n",
       "5    0.0   0  0.9  2.2  13.5   31.1   14  \n",
       "6    0.0   0  0.4  1.0  14.3   20.5    1  \n",
       "7    0.0   0  0.1  0.5  15.8   19.8    1  \n",
       "8    0.0   0  0.6  2.1  12.5   30.9    1  \n",
       "9    0.0   0  0.8  0.9  20.1   25.1   10  \n",
       "10   0.0   0  0.8  0.9  12.3   19.3    3  \n",
       "11   0.0   0  0.5  1.5   4.9   17.2    1  \n",
       "12   0.0   0  0.8  0.5   8.8   12.1   10  \n",
       "13   0.0   0  0.4  2.1  28.5   48.6    6  \n",
       "14   0.0   0  0.3  1.7  39.2   54.1    1  \n",
       "15   0.0   0  0.6  1.7  17.2   31.1    1  \n",
       "16   0.0   0  0.6  2.3  19.5   41.1   10  \n",
       "17   0.0   0  0.3  0.7  17.1   20.8    1  \n",
       "18   0.0   0  0.4  0.5  18.2   22.4    1  \n",
       "19   0.0   0  0.5  1.8  25.2   38.5    1  \n",
       "20   0.0   0  0.6  3.3  17.1   54.7    1  \n",
       "21   0.0   0  0.8  1.1  11.7   19.6    1  \n",
       "22   0.0   0  0.5  1.7  19.7   34.3    1  \n",
       "23   0.0   0  0.4  1.1  18.4   28.9    1  \n",
       "24   0.0   0  0.5  2.5  35.3   57.3    1  \n",
       "25   0.0   0  0.4  1.3  20.7   29.2   16  \n",
       "26   0.0   0  0.6  2.6  44.0   68.4   14  \n",
       "27   0.0   0  0.3  3.2  25.4   54.8   10  \n",
       "28   0.0   0  0.4 -1.4  17.2    3.0    2  \n",
       "29   0.0   0  0.5 -1.1   3.6   -6.3    2  \n",
       "..   ...  ..  ...  ...   ...    ...  ...  \n",
       "422  0.0   0  0.5  3.0  32.7   56.1    1  \n",
       "423  0.0   0  1.0  1.6   9.4   23.4    1  \n",
       "424  0.0   0  1.0  5.5  36.7  115.9    9  \n",
       "425  0.0   0  0.6  3.0  24.1   52.9    1  \n",
       "426  0.0   0  0.5  2.3  20.9   40.6    1  \n",
       "427  0.0   0  0.4  0.8  -2.9    6.5   10  \n",
       "428  0.0   0  0.3  0.4  23.7   26.4    1  \n",
       "429  0.0   0  0.7  3.6  16.1   49.2   16  \n",
       "430  0.0   0  0.2  1.8  12.2   25.1   10  \n",
       "431  0.0   0  0.6  1.2  20.4   29.0    6  \n",
       "432  0.0   0  0.7  5.5  15.1   84.4   10  \n",
       "433  0.0   0  0.3 -0.2  -7.1   -8.3    3  \n",
       "434  0.0   0  0.5  0.4  24.0   25.4    1  \n",
       "435  0.0   0  0.8  1.3  24.1   33.7    1  \n",
       "436  0.0   0  0.5  2.4  19.1   36.3    1  \n",
       "437  0.0   0  1.0  2.1  25.6   43.2    1  \n",
       "438  0.0   0  0.5  0.6  30.1   35.0    1  \n",
       "439  0.0   0  0.8  1.7  21.5   33.7    1  \n",
       "440  0.0   0  0.7  1.8  24.9   41.4    1  \n",
       "441  0.0   0  0.7  1.8  40.1   55.5    1  \n",
       "442  0.0   0  0.5  0.5  -3.8    0.4    1  \n",
       "443  0.0   0  0.5  1.4  17.8   29.5   10  \n",
       "444  0.0   0  0.5  2.3  14.1   37.1    1  \n",
       "445  0.0   0 -0.4  1.3   8.5   17.6    1  \n",
       "446  0.0   0  0.5  2.3  17.6   39.2    1  \n",
       "447  0.0   0  0.7  0.6  -4.4   -0.5    1  \n",
       "448  0.0   0  0.4  2.4  38.0   62.4   10  \n",
       "449  0.0   0  1.5  1.0 -44.2  -33.2    2  \n",
       "450  0.0   0  0.5  2.4  25.0   46.6    1  \n",
       "451  0.0   0  0.5  1.6  21.3   32.8    1  \n",
       "\n",
       "[452 rows x 280 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data=pd.read_csv(\"cardiac_arrhythmia.csv\",header=None)\n",
    "data=Data # Creating Duplicate for the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are 452 Rows and 280 columns in the data. We can say total number of observations are very less when compared to number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in tha Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ?\n",
       "1     ?\n",
       "2    23\n",
       "3     ?\n",
       "4     ?\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[13].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are missing values in tha data but we can find them as \"?\" instead of \"nan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Null values and Replacing the Missing Values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2    23\n",
       "3     0\n",
       "4     0\n",
       "Name: 13, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "for i in data.columns:\n",
    "    data[i]=data[i].replace(\"?\",0)\n",
    "data[13].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.0</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>452.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>46.471239</td>\n",
       "      <td>0.550885</td>\n",
       "      <td>166.188053</td>\n",
       "      <td>68.170354</td>\n",
       "      <td>88.920354</td>\n",
       "      <td>155.152655</td>\n",
       "      <td>367.207965</td>\n",
       "      <td>169.949115</td>\n",
       "      <td>90.004425</td>\n",
       "      <td>33.676991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.278982</td>\n",
       "      <td>9.048009</td>\n",
       "      <td>-1.457301</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.514823</td>\n",
       "      <td>1.222345</td>\n",
       "      <td>19.326106</td>\n",
       "      <td>29.473230</td>\n",
       "      <td>3.880531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.466631</td>\n",
       "      <td>0.497955</td>\n",
       "      <td>37.170340</td>\n",
       "      <td>16.590803</td>\n",
       "      <td>15.364394</td>\n",
       "      <td>44.842283</td>\n",
       "      <td>33.385421</td>\n",
       "      <td>35.633072</td>\n",
       "      <td>25.826643</td>\n",
       "      <td>45.431434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548876</td>\n",
       "      <td>3.472862</td>\n",
       "      <td>2.002430</td>\n",
       "      <td>0.050118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347531</td>\n",
       "      <td>1.426052</td>\n",
       "      <td>13.503922</td>\n",
       "      <td>18.493927</td>\n",
       "      <td>4.407097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>232.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-172.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-28.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>-44.200000</td>\n",
       "      <td>-38.600000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.425000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>-2.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11.450000</td>\n",
       "      <td>17.550000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>-1.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>27.900000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>384.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>25.825000</td>\n",
       "      <td>41.125000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>780.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>524.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>381.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>88.800000</td>\n",
       "      <td>115.900000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5    \\\n",
       "count  452.000000  452.000000  452.000000  452.000000  452.000000  452.000000   \n",
       "mean    46.471239    0.550885  166.188053   68.170354   88.920354  155.152655   \n",
       "std     16.466631    0.497955   37.170340   16.590803   15.364394   44.842283   \n",
       "min      0.000000    0.000000  105.000000    6.000000   55.000000    0.000000   \n",
       "25%     36.000000    0.000000  160.000000   59.000000   80.000000  142.000000   \n",
       "50%     47.000000    1.000000  164.000000   68.000000   86.000000  157.000000   \n",
       "75%     58.000000    1.000000  170.000000   79.000000   94.000000  175.000000   \n",
       "max     83.000000    1.000000  780.000000  176.000000  188.000000  524.000000   \n",
       "\n",
       "              6           7           8           9       ...             270  \\\n",
       "count  452.000000  452.000000  452.000000  452.000000     ...      452.000000   \n",
       "mean   367.207965  169.949115   90.004425   33.676991     ...       -0.278982   \n",
       "std     33.385421   35.633072   25.826643   45.431434     ...        0.548876   \n",
       "min    232.000000  108.000000    0.000000 -172.000000     ...       -4.100000   \n",
       "25%    350.000000  148.000000   79.000000    3.750000     ...       -0.425000   \n",
       "50%    367.000000  162.000000   91.000000   40.000000     ...        0.000000   \n",
       "75%    384.000000  179.000000  102.000000   66.000000     ...        0.000000   \n",
       "max    509.000000  381.000000  205.000000  169.000000     ...        0.000000   \n",
       "\n",
       "              271         272         273    274         275         276  \\\n",
       "count  452.000000  452.000000  452.000000  452.0  452.000000  452.000000   \n",
       "mean     9.048009   -1.457301    0.003982    0.0    0.514823    1.222345   \n",
       "std      3.472862    2.002430    0.050118    0.0    0.347531    1.426052   \n",
       "min      0.000000  -28.600000    0.000000    0.0   -0.800000   -6.000000   \n",
       "25%      6.600000   -2.100000    0.000000    0.0    0.400000    0.500000   \n",
       "50%      8.800000   -1.100000    0.000000    0.0    0.500000    1.350000   \n",
       "75%     11.200000    0.000000    0.000000    0.0    0.700000    2.100000   \n",
       "max     23.600000    0.000000    0.800000    0.0    2.400000    6.000000   \n",
       "\n",
       "              277         278         279  \n",
       "count  452.000000  452.000000  452.000000  \n",
       "mean    19.326106   29.473230    3.880531  \n",
       "std     13.503922   18.493927    4.407097  \n",
       "min    -44.200000  -38.600000    1.000000  \n",
       "25%     11.450000   17.550000    1.000000  \n",
       "50%     18.100000   27.900000    1.000000  \n",
       "75%     25.825000   41.125000    6.000000  \n",
       "max     88.800000  115.900000   16.000000  \n",
       "\n",
       "[8 rows x 275 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Target variable from the Data.\n",
    "#### Input Variable = x \n",
    "#### Target variable = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1]\n",
    "y=data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Target variable for MultipleClass Classiffication and for Detectecting Arrhythmia\n",
    "#### y is for Multiclass Classification\n",
    "#### z is for detecting Arrhythmia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     245\n",
      "10     50\n",
      "2      44\n",
      "6      25\n",
      "16     22\n",
      "4      15\n",
      "3      15\n",
      "5      13\n",
      "9       9\n",
      "15      5\n",
      "14      4\n",
      "7       3\n",
      "8       2\n",
      "Name: 279, dtype: int64\n",
      "False    245\n",
      "True     207\n",
      "Name: 279, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.value_counts())\n",
    "z = y!=1\n",
    "print(z.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is NO class imbalence problem in the dataset.\n",
    "#### There are 207 cases with arrhythmia and 245 records with no arrhythmia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Target variable, there are number of instances related to a person having no Arrhythmia with target label \"1\" and a person with symptoms of Arrhythmia with labels ranging from \"2 to 15\".\n",
    "\n",
    "In the dataset, we have 245 observations of normal person and remaining 207 observations are spread across 14 labelled data. So, there is huge class imbalance problem, when we consider multi-class data.\n",
    "\n",
    "Because of the above reason, I classified the target variable as two categories.\n",
    "1. Obsevation with no symptoms\n",
    "2. Observations with symptoms.\n",
    "\n",
    "By doing this, we can eliminate the class imbalance problem. If the prediction says that an observations is having symptoms, then we can do the predictions on multi-class data to know the related symptoms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training 339\n",
      "Number of rows in testing 113\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=0)\n",
    "x_train, x_test, z_train, z_test= train_test_split(x,z,random_state=0)\n",
    "print(\"Number of rows in training\", x_train[0].count())\n",
    "print(\"Number of rows in testing\", x_test[0].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_trainscaled = scaler.fit_transform(x_train)\n",
    "x_testscaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data is very important to increase the speed of learning rate. Initially the data is spread across different ranges forming eliptical curves during the optimization of the gradient decent. By applying the scaling function we can get all the columns in a simillar range which helps the gradient decent to optimize the values of the coefficient in a very few number of steps and eliminate the changes of strucking in a local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with Multiclass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.7080\n",
      "The best test score is :  0.6106\n",
      "{'n_neighbors': 2}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='uniform')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[58,  1,  0,  1,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  4,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [10,  1,  0,  0,  1,  0,  0,  0,  1,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnclas=KNeighborsClassifier()\n",
    "k_value={'n_neighbors':range(1,100)}\n",
    "grid=GridSearchCV(knnclas,param_grid=k_value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "knnclasp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,knnclasp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn Training scaore 0.70796460177\n",
      "knn Test score 0.610619469027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[58,  1,  0,  1,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  4,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [10,  1,  0,  0,  1,  0,  0,  0,  1,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnm=KNeighborsClassifier(n_neighbors=2)\n",
    "knn=knnm.fit(x_trainscaled,y_train)\n",
    "print(\"knn Training scaore\", knn.score(x_trainscaled,y_train))\n",
    "print(\"knn Test score\",knn.score(x_testscaled,y_test))\n",
    "knnp=knn.predict(x_testscaled)\n",
    "confusion_matrix(y_test,knnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of KNN: 0.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of KNN:\", \n",
    "      '%.4f' %cross_val_score(KNeighborsClassifier(n_neighbors=2), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with Arrhythmia detection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  1.0000\n",
      "The best test score is :  0.6372\n",
      "{'n_neighbors': 1}\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50, 11],\n",
       "       [30, 22]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knnclas=KNeighborsClassifier()\n",
    "k_value={'n_neighbors':range(1,100)}\n",
    "grid=GridSearchCV(knnclas,param_grid=k_value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "knnclasp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,knnclasp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn Training scaore 1.0\n",
      "knn Test score 0.637168141593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50, 11],\n",
       "       [30, 22]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knna=KNeighborsClassifier(n_neighbors=1)\n",
    "knn=knna.fit(x_trainscaled,z_train)\n",
    "print(\"knn Training scaore\", knn.score(x_trainscaled,z_train))\n",
    "print(\"knn Test score\",knn.score(x_testscaled,z_test))\n",
    "knnp=knn.predict(x_testscaled)\n",
    "confusion_matrix(z_test,knnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of KNN: 0.6723\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of KNN:\", \n",
    "      '%.4f' %cross_val_score(KNeighborsClassifier(n_neighbors=1), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is known for its memory. It remembers whole training data, which results in maximum training accuracy. But it is performing very poorly on the test data. So, we can expect that the number training examples is not enough for KNN. Because we have only 452 observations. So, knn is ruled out of our consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8496\n",
      "The best test score is :  0.6814\n",
      "{'C': 2, 'penalty': 'l1'}\n",
      "LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[56,  3,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 7,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 3,  1,  0,  0,  0,  0,  0,  0,  9,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrg=LogisticRegression()\n",
    "value={'penalty':['l1','l2'], 'C':[0.1,0.5,0.75,1,2,5,10,15,20,30,50,100]}\n",
    "grid=GridSearchCV(lrg,param_grid=value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "lrgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,lrgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistics Training score 0.849557522124\n",
      "Logistics Test Score 0.681415929204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[56,  3,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 7,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0],\n",
       "       [ 3,  1,  0,  0,  0,  0,  0,  0,  9,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  1,  0,  0,  0,  0,  0,  0,  0,  1,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm=LogisticRegression(C=2,penalty='l1')\n",
    "lr=lrm.fit(x_trainscaled,y_train)\n",
    "print(\"Logistics Training score\", lr.score(x_trainscaled,y_train))\n",
    "print(\"Logistics Test Score\",lr.score(x_testscaled,y_test))\n",
    "lrp=lr.predict(x_testscaled)\n",
    "confusion_matrix(y_test,lrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Logistic Regression: 0.7063\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Logistic Regression:\", \n",
    "      '%.4f' %cross_val_score(LogisticRegression(C=0.75,penalty='l2'), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to the Logistic regression, we can observe that the testing accuracy on multiclass data is little better than KNN model, but we cannot say the best because it is clearly overfitting the data. The best model is obtained at c=0.75 with a penalty of L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8525\n",
      "The best test score is :  0.7168\n",
      "{'C': 0.75, 'penalty': 'l2'}\n",
      "LogisticRegression(C=0.75, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50, 11],\n",
       "       [21, 31]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrg=LogisticRegression()\n",
    "value={'penalty':['l1','l2'], 'C':[0.1,0.5,0.75,1,2,5,10,15,20,30,50,100]}\n",
    "grid=GridSearchCV(lrg,param_grid=value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "lrgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,lrgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistics Training score 0.852507374631\n",
      "Logistics Test Score 0.716814159292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50, 11],\n",
       "       [21, 31]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lra=LogisticRegression(C=0.75,penalty='l2')\n",
    "lr=lra.fit(x_trainscaled,z_train)\n",
    "print(\"Logistics Training score\", lr.score(x_trainscaled,z_train))\n",
    "print(\"Logistics Test Score\",lr.score(x_testscaled,z_test))\n",
    "lrp=lr.predict(x_testscaled)\n",
    "confusion_matrix(z_test,lrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Logistic Regression: 0.7904\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Logistic Regression:\", \n",
    "      '%.4f' %cross_val_score(LogisticRegression(C=0.75,penalty='l2'), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the Multiclass data, we can say that logistic regression is performing better in detection of Arrhythmia resulting a cross validation score close to 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8909\n",
      "The best test score is :  0.6814\n",
      "{'C': 0.3}\n",
      "LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[56,  3,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  0,  0,  9,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  1,  0,  0,  1,  0,  0,  0,  0,  1,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcg=LinearSVC()\n",
    "value={'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.75,1,2,5,10,15,20,30,50,100]}\n",
    "grid=GridSearchCV(svcg,param_grid=value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "svcgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,svcgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Training Score 0.890855457227\n",
      "The Best Test Score 0.681415929204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[56,  3,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0],\n",
       "       [ 5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  0,  0,  9,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  1,  0,  0,  1,  0,  0,  0,  0,  1,  0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svclm=LinearSVC(C=0.3)\n",
    "svc=svclm.fit(x_trainscaled,y_train)\n",
    "print(\"The Best Training Score\",svc.score(x_trainscaled,y_train))\n",
    "print(\"The Best Test Score\",svc.score(x_testscaled,y_test))\n",
    "svcp=svc.predict(x_testscaled)\n",
    "confusion_matrix(y_test,svcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Linear SVC: 0.7298\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Linear SVC:\", \n",
    "      '%.4f' %cross_val_score(LinearSVC(C=0.3), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Arrhythmia detection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8643\n",
      "The best test score is :  0.7257\n",
      "{'C': 0.15}\n",
      "LinearSVC(C=0.15, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50, 11],\n",
       "       [20, 32]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcg=LinearSVC()\n",
    "value={'C':[0.05,0.1,0.15,0.4,0.5,0.75,1,2,5,10,15,20,30,50,100]}\n",
    "grid=GridSearchCV(svcg,param_grid=value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "svcgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,svcgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.864306784661\n",
      "The best Test Score is : 0.725663716814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[50, 11],\n",
       "       [20, 32]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcla=LinearSVC(C=0.15)\n",
    "svc=svcla.fit(x_trainscaled,z_train)\n",
    "print(\"The best Training score is :\", svc.score(x_trainscaled,z_train))\n",
    "print(\"The best Test Score is :\",svc.score(x_testscaled,z_test))\n",
    "svcp=svc.predict(x_testscaled)\n",
    "confusion_matrix(z_test,svcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Linear SVC: 0.7756\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Linear SVC:\", \n",
    "      '%.4f' %cross_val_score(LinearSVC(C=0.15), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear support vector machines is kind of overfitting in both cases but comparitively it is performing best in identifying the Arrhythmia. But we cannot say that it is the best, but still standing in the race with a good cross validation score of 77.56%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kerenilzed Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultiClass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8024\n",
      "The best test score is :  0.6726\n",
      "{'C': 12, 'gamma': 0.01}\n",
      "SVC(C=12, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[58,  2,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0],\n",
       "       [ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcg=SVC(kernel='rbf')\n",
    "value = {'C':[0.1,1,5,10,12,15,17,20,25], 'gamma':[0.01,0.05,0.1,0.25,0.5, 1, 5]}\n",
    "grid=GridSearchCV(svcg,param_grid=value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "svcgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,svcgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.802359882006\n",
      "The best Test Score is : 0.672566371681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[58,  2,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0],\n",
       "       [ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcrm=SVC(kernel='rbf',C=12,gamma=0.01)\n",
    "svc=svcrm.fit(x_trainscaled,y_train)\n",
    "print(\"The best Training score is :\", svc.score(x_trainscaled,y_train))\n",
    "print(\"The best Test Score is :\",svc.score(x_testscaled,y_test))\n",
    "svcp=svc.predict(x_testscaled)\n",
    "confusion_matrix(y_test,svcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Kernalized SVC: 0.7144\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Kernalized SVC:\", \n",
    "      '%.4f' %cross_val_score(SVC(kernel='rbf',C=12,gamma=0.01), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8938\n",
      "The best test score is :  0.7522\n",
      "{'C': 5, 'gamma': 0.05}\n",
      "SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.05, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51, 10],\n",
       "       [18, 34]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcg=SVC(kernel='rbf')\n",
    "value = {'C':[0.1,1,3,5,7,10], 'gamma':[0.01,0.05,0.1,0.25,0.5, 1, 5]}\n",
    "grid=GridSearchCV(svcg,param_grid=value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "svcgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,svcgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.893805309735\n",
      "The best Test Score is : 0.752212389381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51, 10],\n",
       "       [18, 34]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcra=SVC(kernel='rbf',C=5,gamma=0.05)\n",
    "svc=svcra.fit(x_trainscaled,z_train)\n",
    "print(\"The best Training score is :\", svc.score(x_trainscaled,z_train))\n",
    "print(\"The best Test Score is :\",svc.score(x_testscaled,z_test))\n",
    "svcp=svc.predict(x_testscaled)\n",
    "confusion_matrix(z_test,svcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Kernalized SVC: 0.7818\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Kernalized SVC:\", \n",
    "      '%.4f' %cross_val_score(SVC(kernel='rbf',C=5,gamma=0.05), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8142\n",
      "The best test score is :  0.6903\n",
      "{'C': 1, 'degree': 2, 'gamma': 0.05}\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[58,  2,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  0,  0,  0,  8,  0,  0],\n",
       "       [ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcg=SVC(kernel='poly')\n",
    "value = {'degree':[2,3,4,5],'C':[0.1,0.5,1,2,5,10], 'gamma':[0.01,0.05,0.1,0.25,0.5, 1, 5]}\n",
    "grid=GridSearchCV(svcg,param_grid=value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "svcgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,svcgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.814159292035\n",
      "The best Test Score is : 0.690265486726\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[58,  2,  0,  0,  0,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  2,  0,  0,  0,  1,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 5,  0,  0,  0,  0,  0,  0,  0,  8,  0,  0],\n",
       "       [ 2,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcpm=SVC(kernel='poly',degree=2,C=1,gamma=0.05)\n",
    "svc=svcpm.fit(x_trainscaled,y_train)\n",
    "print(\"The best Training score is :\", svc.score(x_trainscaled,y_train))\n",
    "print(\"The best Test Score is :\",svc.score(x_testscaled,y_test))\n",
    "svcp=svc.predict(x_testscaled)\n",
    "confusion_matrix(y_test,svcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Kernalized SVC: 0.7141\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Kernalized SVC:\", \n",
    "      '%.4f' %cross_val_score(SVC(kernel='poly',degree=2,C=1,gamma=0.05), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8761\n",
      "The best test score is :  0.7168\n",
      "{'C': 0.5, 'degree': 3, 'gamma': 0.05}\n",
      "SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[49, 12],\n",
       "       [20, 32]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcg=SVC(kernel='poly')\n",
    "value = {'degree':[2,3,4,5],'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,1,2,5,10], 'gamma':[0.01,0.05,0.1,0.25,0.5, 1, 5]}\n",
    "grid=GridSearchCV(svcg,param_grid=value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "svcgp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,svcgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.87610619469\n",
      "The best Test Score is : 0.716814159292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[49, 12],\n",
       "       [20, 32]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svcpa=SVC(kernel='poly',degree=3,C=0.5,gamma=0.05)\n",
    "svc=svcpa.fit(x_trainscaled,z_train)\n",
    "print(\"The best Training score is :\", svc.score(x_trainscaled,z_train))\n",
    "print(\"The best Test Score is :\",svc.score(x_testscaled,z_test))\n",
    "svcp=svc.predict(x_testscaled)\n",
    "confusion_matrix(z_test,svcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Kernalized SVC: 0.7642\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Kernalized SVC:\", \n",
    "      '%.4f' %cross_val_score(SVC(kernel='poly',degree=3,C=0.5,gamma=0.05), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not much improvement in the kernelized support vector in both RBF and Polynomial kernel. Both the cases are overfitting in classifying multi-class data as well as detectin Arrhythmia. But the cross validation scores are satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.8555\n",
      "The best test score is :  0.6018\n",
      "{'max_depth': 6}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51,  4,  1,  1,  0,  1,  0,  0,  3,  0,  0],\n",
       "       [ 4,  3,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  1,  0,  0,  0,  1,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  5,  0,  0,  1,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  1,  0,  0,  0,  0,  0,  7,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1],\n",
       "       [ 4,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier()\n",
    "value={\"max_depth\":range(1,20)}\n",
    "grid=GridSearchCV(dt,param_grid=value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "dtp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,dtp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.855457227139\n",
      "The best Test Score is : 0.601769911504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[51,  5,  1,  0,  1,  1,  0,  0,  2,  0,  0,  0],\n",
       "       [ 5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 2,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1],\n",
       "       [ 1,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  1,  0,  0,  0,  0,  0,  7,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  1,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm=DecisionTreeClassifier(max_depth = 6)\n",
    "dt=dtm.fit(x_trainscaled,y_train)\n",
    "print(\"The best Training score is :\", dt.score(x_trainscaled,y_train))\n",
    "print(\"The best Test Score is :\",dt.score(x_testscaled,y_test))\n",
    "dtp=dt.predict(x_testscaled)\n",
    "confusion_matrix(y_test,dtp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Decision Tree is : 0.6955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Decision Tree is :\", \n",
    "      '%.4f' %cross_val_score(DecisionTreeClassifier(max_depth = 6), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.9027\n",
      "The best test score is :  0.7257\n",
      "{'max_depth': 5}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[48, 13],\n",
       "       [18, 34]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier()\n",
    "value={\"max_depth\":range(1,20)}\n",
    "grid=GridSearchCV(dt,param_grid=value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "dtp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,dtp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.93215339233\n",
      "The best Test Score is : 0.699115044248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[43, 18],\n",
       "       [16, 36]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta=DecisionTreeClassifier(max_depth = 6)\n",
    "dt=dta.fit(x_trainscaled,z_train)\n",
    "print(\"The best Training score is :\", dt.score(x_trainscaled,z_train))\n",
    "print(\"The best Test Score is :\",dt.score(x_testscaled,z_test))\n",
    "dtp=dt.predict(x_testscaled)\n",
    "confusion_matrix(z_test,dtp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Decision Tree is : 0.7604\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Decision Tree is :\", \n",
    "      '%.4f' %cross_val_score(DecisionTreeClassifier(max_depth = 6), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are performing well in dsetection of Arrhythmia but failing to detect the exact reason for Arrhythmia and the best scores are observed at a tree depth of 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  0.9971\n",
      "The best test score is :  0.6726\n",
      "{'n_estimators': 35}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=35, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[55,  2,  0,  0,  0,  1,  0,  0,  3,  0,  0],\n",
       "       [ 5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  1,  0,  0,  0,  2,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  0,  0,  9,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state = 0)\n",
    "value={\"n_estimators\":range(1,40)}\n",
    "grid=GridSearchCV(rf,param_grid=value)\n",
    "grid.fit(x_trainscaled,y_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,y_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,y_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "rfp=grid.predict(x_testscaled)\n",
    "confusion_matrix(y_test,rfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 0.997050147493\n",
      "The best Test Score is : 0.672566371681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[55,  2,  0,  0,  0,  1,  0,  0,  3,  0,  0],\n",
       "       [ 5,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  5,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  1,  0,  0,  0,  2,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  1,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  0,  0,  9,  0,  0],\n",
       "       [ 3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfm = RandomForestClassifier(n_estimators=35,random_state = 0)\n",
    "rf=rfm.fit(x_trainscaled,y_train)\n",
    "print(\"The best Training score is :\", rf.score(x_trainscaled,y_train))\n",
    "print(\"The best Test Score is :\",rf.score(x_testscaled,y_test))\n",
    "rfp=rf.predict(x_testscaled)\n",
    "confusion_matrix(y_test,rfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Random Forest is : 0.7340\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Random Forest is :\", \n",
    "      '%.4f' %cross_val_score(RandomForestClassifier(n_estimators=35,random_state = 0), x_trainscaled, y_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best training score is :  1.0000\n",
      "The best test score is :  0.7257\n",
      "{'n_estimators': 30}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=30, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[47, 14],\n",
       "       [17, 35]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state = 0)\n",
    "value={\"n_estimators\":range(1,40)}\n",
    "grid=GridSearchCV(rf,param_grid=value)\n",
    "grid.fit(x_trainscaled,z_train)\n",
    "print(\"The best training score is : \",'%.4f' %grid.score(x_trainscaled,z_train))\n",
    "print(\"The best test score is : \",'%.4f' %grid.score(x_testscaled,z_test))\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "rfp=grid.predict(x_testscaled)\n",
    "confusion_matrix(z_test,rfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best Training score is : 1.0\n",
      "The best Test Score is : 0.725663716814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[47, 14],\n",
       "       [17, 35]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfa = RandomForestClassifier(n_estimators=30,random_state = 0)\n",
    "rf=rfa.fit(x_trainscaled,z_train)\n",
    "print(\"The best Training score is :\", rf.score(x_trainscaled,z_train))\n",
    "print(\"The best Test Score is :\",rf.score(x_testscaled,z_test))\n",
    "rfp=rf.predict(x_testscaled)\n",
    "confusion_matrix(z_test,rfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Cross Validation Score of Random Forest is : 0.8088\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Cross Validation Score of Random Forest is :\", \n",
    "      '%.4f' %cross_val_score(RandomForestClassifier(n_estimators=30,random_state = 0), x_trainscaled, z_train, cv = 10).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the Random Forest came into the race with highest cross validation score of 81% in detecting the Arrhythmia. This is obtained with the 30 estimators with a random state of 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='uniform')\n",
      "training score: 0.619469026549\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n",
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.705014749263\n",
      "testing score: 0.646017699115\n",
      "\n",
      "\n",
      "model: LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.890855457227\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "model: SVC(C=12, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.598820058997\n",
      "testing score: 0.566371681416\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.598820058997\n",
      "testing score: 0.58407079646\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "training score: 0.837758112094\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=35, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 0.758112094395\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [knnm,lrm,svclm,svcrm,svcpm,dtm,rfm]\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "for model in models:\n",
    "    bag = BaggingClassifier(model,n_estimators=100,max_samples=100,bootstrap=True,n_jobs=-1,random_state=10)\n",
    "    bag.fit(x_trainscaled,y_train)\n",
    "    print('model:',model)\n",
    "    print('training score:',bag.score(x_trainscaled,y_train))\n",
    "    print('testing score:',bag.score(x_testscaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying bagging on multiclass data, we can observe that there is a lot of improvement in training the model. It clearly tried hard to reduce the overfitting but failed to obtain the best model.\n",
    "The only model that got benefitted from bagging is Decision Trees, where you can observe a clear increaase of 10% in testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
      "           weights='uniform')\n",
      "training score: 0.619469026549\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n",
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.705014749263\n",
      "testing score: 0.646017699115\n",
      "\n",
      "\n",
      "model: LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.890855457227\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "model: SVC(C=12, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.598820058997\n",
      "testing score: 0.566371681416\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.598820058997\n",
      "testing score: 0.58407079646\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "training score: 0.837758112094\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=35, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 0.758112094395\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [knnm,lrm,svclm,svcrm,svcpm,dtm,rfm]\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "for model in models:\n",
    "    bag = BaggingClassifier(model,n_estimators=100,max_samples=100,bootstrap=True,n_jobs=-1,random_state=10)\n",
    "    bag.fit(x_trainscaled,y_train)\n",
    "    print('model:',model)\n",
    "    print('training score:',bag.score(x_trainscaled,y_train))\n",
    "    print('testing score:',bag.score(x_testscaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Decision Trees got benefitted by applying bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.890855457227\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "model: SVC(C=12, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "training score: 1.0\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=35, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 1.0\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [lrm,svclm,svcrm,svcpm,dtm,rfm]\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for model in models:\n",
    "    ada = AdaBoostClassifier(model,n_estimators=100,algorithm='SAMME',learning_rate=0.5,random_state=42)\n",
    "    ada.fit(x_trainscaled,y_train)\n",
    "    print('model:',model)\n",
    "    print('training score:',ada.score(x_trainscaled,y_train))\n",
    "    print('testing score:', ada.score(x_testscaled,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.896755162242\n",
      "testing score: 0.743362831858\n",
      "\n",
      "\n",
      "model: SVC(C=12, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "training score: 1.0\n",
      "testing score: 0.823008849558\n",
      "\n",
      "\n",
      "model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=35, n_jobs=1, oob_score=False, random_state=0,\n",
      "            verbose=0, warm_start=False)\n",
      "training score: 1.0\n",
      "testing score: 0.787610619469\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [lrm,svclm,svcrm,svcpm,dtm,rfm]\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "for model in models:\n",
    "    ada = AdaBoostClassifier(model,n_estimators=100,algorithm='SAMME',learning_rate=0.5,random_state=42)\n",
    "    ada.fit(x_trainscaled,z_train)\n",
    "    print('model:',model)\n",
    "    print('training score:',ada.score(x_trainscaled,z_train))\n",
    "    print('testing score:', ada.score(x_testscaled,z_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.\n",
    "This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n",
    "AdaBoost is best used to boost the performance of decision trees on binary classification problems.\n",
    "\n",
    "We can clearly observe that Adaboost failed in improving the prediction on multiclass data, but huge success in improving the binary data using the decision trees. \n",
    "\n",
    "The performance of decision tree increased by 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1\n",
      "learning rate: 0.001\n",
      "training score: 0.663716814159\n",
      "testing score: 0.592920353982\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.01\n",
      "training score: 0.843657817109\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.1\n",
      "training score: 0.979351032448\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.001\n",
      "training score: 0.749262536873\n",
      "testing score: 0.601769911504\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.01\n",
      "training score: 0.914454277286\n",
      "testing score: 0.716814159292\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.725663716814\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.001\n",
      "training score: 0.834808259587\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.01\n",
      "training score: 0.952802359882\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.001\n",
      "training score: 0.87610619469\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.01\n",
      "training score: 0.979351032448\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.716814159292\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.001\n",
      "training score: 0.920353982301\n",
      "testing score: 0.646017699115\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.01\n",
      "training score: 0.988200589971\n",
      "testing score: 0.681415929204\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.734513274336\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.001\n",
      "training score: 0.926253687316\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.01\n",
      "training score: 0.991150442478\n",
      "testing score: 0.70796460177\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.001\n",
      "training score: 0.952802359882\n",
      "testing score: 0.637168141593\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.01\n",
      "training score: 0.997050147493\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.001\n",
      "training score: 0.967551622419\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.01\n",
      "training score: 0.997050147493\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.001\n",
      "training score: 0.973451327434\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.01\n",
      "training score: 1.0\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.70796460177\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "for depth in range(1,10):\n",
    "    for rate in (0.001,0.01,0.1):\n",
    "        gbModel = GradientBoostingClassifier(max_depth=depth,learning_rate=rate,random_state=0)\n",
    "        gbModel.fit(x_trainscaled,y_train)\n",
    "        print('depth:',depth)\n",
    "        print('learning rate:',rate)\n",
    "        print('training score:',gbModel.score(x_trainscaled,y_train))\n",
    "        print('testing score:',gbModel.score(x_testscaled,y_test))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting increased the performance of model on multiclass data by 15% at a depth of 5 with a learning rate of 0.1. But still it is overfitting because the training error is 0 but the testing error is still 25% even it is the best model sofar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arrhythmia detection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1\n",
      "learning rate: 0.001\n",
      "training score: 0.542772861357\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.01\n",
      "training score: 0.784660766962\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 1\n",
      "learning rate: 0.1\n",
      "training score: 0.920353982301\n",
      "testing score: 0.823008849558\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.001\n",
      "training score: 0.678466076696\n",
      "testing score: 0.628318584071\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.01\n",
      "training score: 0.867256637168\n",
      "testing score: 0.752212389381\n",
      "\n",
      "\n",
      "depth: 2\n",
      "learning rate: 0.1\n",
      "training score: 0.982300884956\n",
      "testing score: 0.83185840708\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.001\n",
      "training score: 0.722713864307\n",
      "testing score: 0.619469026549\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.01\n",
      "training score: 0.923303834808\n",
      "testing score: 0.769911504425\n",
      "\n",
      "\n",
      "depth: 3\n",
      "learning rate: 0.1\n",
      "training score: 0.997050147493\n",
      "testing score: 0.83185840708\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.001\n",
      "training score: 0.793510324484\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.01\n",
      "training score: 0.946902654867\n",
      "testing score: 0.787610619469\n",
      "\n",
      "\n",
      "depth: 4\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.823008849558\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.001\n",
      "training score: 0.879056047198\n",
      "testing score: 0.690265486726\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.01\n",
      "training score: 0.961651917404\n",
      "testing score: 0.796460176991\n",
      "\n",
      "\n",
      "depth: 5\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.796460176991\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.001\n",
      "training score: 0.914454277286\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.01\n",
      "training score: 0.973451327434\n",
      "testing score: 0.787610619469\n",
      "\n",
      "\n",
      "depth: 6\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.83185840708\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.001\n",
      "training score: 0.946902654867\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.01\n",
      "training score: 0.976401179941\n",
      "testing score: 0.743362831858\n",
      "\n",
      "\n",
      "depth: 7\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.814159292035\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.001\n",
      "training score: 0.958702064897\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.01\n",
      "training score: 0.985250737463\n",
      "testing score: 0.769911504425\n",
      "\n",
      "\n",
      "depth: 8\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.699115044248\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.001\n",
      "training score: 0.964601769912\n",
      "testing score: 0.663716814159\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.01\n",
      "training score: 0.988200589971\n",
      "testing score: 0.70796460177\n",
      "\n",
      "\n",
      "depth: 9\n",
      "learning rate: 0.1\n",
      "training score: 1.0\n",
      "testing score: 0.752212389381\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "for depth in range(1,10):\n",
    "    for rate in (0.001,0.01,0.1):\n",
    "        gbModel = GradientBoostingClassifier(max_depth=depth,learning_rate=rate,random_state=0)\n",
    "        gbModel.fit(x_trainscaled,z_train)\n",
    "        print('depth:',depth)\n",
    "        print('learning rate:',rate)\n",
    "        print('training score:',gbModel.score(x_trainscaled,z_train))\n",
    "        print('testing score:',gbModel.score(x_testscaled,z_test))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score: 0.920353982301\n",
      "testing score: 0.823008849558\n"
     ]
    }
   ],
   "source": [
    "gbModel = GradientBoostingClassifier(max_depth=1,learning_rate=0.1,random_state=0)\n",
    "gbModel.fit(x_trainscaled,z_train)\n",
    "print('training score:',gbModel.score(x_trainscaled,z_train))\n",
    "print('testing score:',gbModel.score(x_testscaled,z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is doing awesome job at a depth of 3 with a learning rate of 0.1 resulting a 25% increase in testing accuracy and even the model looks good as the training error is 8% and the testing error is 14%. \n",
    "So, We can say that the gradient boosting improving the results by a great extend and standing as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principle component analysis is mainly used for dimentionality reduction by preserving the varience in trhe dataset. Here we have 270 feature which are making the model to overfit. So, we try to reduce the number of features to 50 by applying the principle component analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15346672  0.10006548  0.09420965  0.0845956   0.05312036  0.05062328\n",
      "  0.04585218  0.04113792  0.03395551  0.0291003   0.02830696  0.02249531\n",
      "  0.02190761  0.01932186  0.01731159  0.01539366  0.01445182  0.01225009\n",
      "  0.01176798  0.00914527  0.00867875  0.00766778  0.00722819  0.00669843\n",
      "  0.0063108   0.00584685  0.0054186   0.00527819  0.0049771   0.00478087\n",
      "  0.00445785  0.00419111  0.00401413  0.00373631  0.00351611  0.00327154\n",
      "  0.00305062  0.00299937  0.00286984  0.00253784  0.00243258  0.00230367\n",
      "  0.00220401  0.00216197  0.00209371  0.00198413  0.00195335  0.00172184\n",
      "  0.00159579  0.00145356]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(452, 50)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 50)\n",
    "datapca = pca.fit_transform(x)\n",
    "print(pca.explained_variance_ratio_)\n",
    "datapca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.917404129794\n",
      "testing score: 0.575221238938\n",
      "\n",
      "\n",
      "model: LinearSVC(C=0.3, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.672566371681\n",
      "testing score: 0.486725663717\n",
      "\n",
      "\n",
      "model: SVC(C=12, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=2, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.530973451327\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "training score: 0.82005899705\n",
      "testing score: 0.548672566372\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca_train,pca_test,y_train,y_test=train_test_split(datapca,y,random_state=0)\n",
    "\n",
    "models = [lrm,svclm,svcrm,svcpm,dtm,rfm]\n",
    "for i in range(5):\n",
    "    models[i].fit(pca_train,y_train)\n",
    "    print('model:',models[i])\n",
    "    print('training score:',models[i].score(pca_train,y_train))\n",
    "    print('testing score:',models[i].score(pca_test,y_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: LogisticRegression(C=0.75, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "training score: 0.870206489676\n",
      "testing score: 0.743362831858\n",
      "\n",
      "\n",
      "model: LinearSVC(C=0.15, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "training score: 0.787610619469\n",
      "testing score: 0.654867256637\n",
      "\n",
      "\n",
      "model: SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.05, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.53982300885\n",
      "\n",
      "\n",
      "model: SVC(C=0.5, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.05, kernel='poly',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "training score: 1.0\n",
      "testing score: 0.672566371681\n",
      "\n",
      "\n",
      "model: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "training score: 0.879056047198\n",
      "testing score: 0.619469026549\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pca_train,pca_test,z_train,z_test=train_test_split(datapca,z,random_state=0)\n",
    "\n",
    "models = [lra,svcla,svcra,svcpa,dta,rfa]\n",
    "for i in range(5):\n",
    "    models[i].fit(pca_train,z_train)\n",
    "    print('model:',models[i])\n",
    "    print('training score:',models[i].score(pca_train,z_train))\n",
    "    print('testing score:',models[i].score(pca_test,z_test))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying Principle component Analysis, we observed that there is total reduce in the performance in all the model. Main reason for this is decrease in number of features in training the model results in failing the model in capturing the original target label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. By applyting different models on the data, we observed the variations in training and testing scores across different model. Random Forest is the best model among all the model with a Cross Validation score of 81%.\n",
    "2. Bagging succeded in reducing the overfit of different models but completely failed in improving the performance of model. The only model that got benefitted is Decision Tree with an increase in testing accuracy of 10%.\n",
    "3. Gradinet boosting done a great job in improving the performance of the decision tree model and got highest accuracy of 84% on testing data, training accuracy of 92% resulting in a good fit.\n",
    "4. After applying Principle component analysis, we reduced the number of features to 50 by preserving more than 95% of its variance. But we already know that by reducing the number of features, we loose information resulting in insufficient data. Because of this there is huge decrese in the accurcy levels across all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
